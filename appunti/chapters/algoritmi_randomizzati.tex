\section{Uso della probabilità negli algoritmi}
% pag 122

\subsection{Nell'analisi}
% pag 122

Nell'analisi probabilistica lo spazio di probabilità $\Omega$ che sottende l'analisi di probabilità non sono le scelte casuali fatte dall'algoritmo, perché l'algoritmo non ne fa, ma è uno spazio di probabilità sugli ingressi dell'algoritmo.

Se gli spazi di probabilità sono suddivisi rispetto alla taglia dell'input, $
\Omega_n
$ è uno spazio di probabilità su $
\bi{}_n
$, ovvero l'insieme delle istanze di taglia $n$.
\begin{equation*}
    \Omega_n
    =
    \bi{}_n
    =
    \left\{ 
        i \in \bi{} : |i| = n
    \right\}
\end{equation*}
Lo spazio delle probabilità sono tutte le possibili istanze, e ogni istanza ha una certa probabilità di accadere.
Si studia l'algoritmo deterministico su una particolare distribuzione degli ingressi.
Nel caso più semplice, la distribuzione è uniforme.

% A questo punto diventa tutto una variabile aleatoria.
Se si suppone di dover studiare l'algoritmo su un input estratto a caso, ci sono parecchie variabili aleatorie rilevanti.
In particolare, la probabilità di errore viene studiata in base all'istanza d'ingresso:
\begin{equation*}
    Pr \left( 
        \text{$A$ sia corretto su $
            \bm{i}
            , 
            |
            \bm{i}
            | = n$}
    \right)
\end{equation*}
E per esempio grazie a Pomerance, per il test di primalità vale 
$
\lim_{n \to \infty} p_n = 1
$.

Ci possono essere di casi in cui l'analisi probabilistica viene fatta sul tempo di esecuzione dell'algoritmo:
la performance al caso peggiore cerca l'istanza critica per cui l'algoritmo impiega più tempo, 
ma si può supporre che gli input siano estratti uniformemente dallo spazio delle istanze
e si vuole studiare l'algoritmo deterministico e la sua performance media su un input estratto casualmente dallo spazio.
\\
Anche la complessità diventa allora una variabile aleatoria, e si studia il comportamento di $
\bm{T}_A
$ su una certa istanza (anch'essa aleatoria) $
\bm{i}
$:
\begin{equation*}
    Pr \left( 
        \text{$
            \bm{T}_A \left( 
                \bm{i}
            \right)
            \leq f (n)
            , \,
            |
            \bm{i}
            | = n
        $}
    \right)
\end{equation*}
Che è una sorta di proprietà distribuzionale di $
\bm{T}_A \left( 
    \bm{i}
\right)
$.
\\
Per esempio, quicksort è un algoritmo deterministico, ma se lo si studia su sequenze scelte a caso, si dimostra che al caso medio ha complessità $
O \left( n \log n \right)
$ invece di $
O \left( n^2 \right)
$ che risulta avere al caso peggiore.

L'analisi al caso medio è proprio l'analisi probabilistica di un algoritmo deterministico.
Se $
\bm{i}_n
$ è un'istanza di taglia $n$ in $
\bi{}_n
$, la media del tempo di esecuzione su istanze $
\bm{i}_n
$ estratte a caso.
\begin{equation*}
    \E{
        T_A \left( 
            \bm{i}_n
        \right)
    }
\end{equation*}
Questa analisi fa un'assunzione molto forte, dovendo conoscere lo spazio di probabilità e la probabilità degli eventi elementari, che molto spesso si assume uniforme.
Se si deve estrarre un numero primo casuale questo è ragionevole, ma se per esempio si devono ordinare dati, l'assunzione sul loro ordinamento casuale è molto forte, e in genere non accurata, dato che in pratica i dati sono spesso già ordinati a gruppi.
La permutazione di ingressi quindi non è casuale, e in più quicksort (nella versione che sceglie il primo elemento come pivot) performa molto male su dati ordinati, rendendo l'analisi particolarmente inefficace.

\subsection{Nell'algoritmo}
% pag 122.7

Lo spazio di probabilità viene creato dall'algoritmo stesso:
l'insieme $
\Omega_n
$ è formato delle scelte casuali fatte dall'algoritmo.
Lo spazio non è basato sugli ingressi, perché in un algoritmo randomizzato si considera un ingresso fissato.
\begin{equation*}
    Pr \left( 
        \text{$A$ sia corretto su un input \emph{fissato}}
    \right)
\end{equation*}
Scelto un input, si studia la probabilità della corettezza al caso peggiore dell'algoritmo.
Si cerca quindi l'input su cui l'algoritmo sbaglia di più.

Anche in questo caso la  complessità può diventare una variabile aleatoria.
\begin{align*}
    Pr \left( 
        \bm{T}_A \left( 
            n
        \right)
        \geq c \cdot f(n)
    \right)
    &
    \leq d(n)
    \\
    Pr \left( 
        \bm{T}_A \left( 
            n
        \right)
        \leq c \cdot f(n)
    \right)
    &
    \geq d'(n)
\end{align*}
Per cui si studia la distribuzione del tempo di esecuzione.

\subsubsection{Algoritmi Las Vegas}
% pag 123
Gli algoritmi di tipo \emph{Las Vegas} sono algoritmi randomizzati \emph{sempre} corretti.
Dato un algoritmo $
A_{\bpi{}} \left( i \right) \to s
$, questo ritorna sempre la soluzione esatta.
\begin{equation*}
    Pr \left( 
        i
        \,
        \bpi{}
        \,
        s
    \right)
    = 1
\end{equation*}
La randomizzazione viene utilizzata per rendere aleatorio il tempo di esecuzione
\begin{align*}
    Pr \left( 
        \bm{T}_A \left( 
            n
        \right)
        \leq c \cdot f(n)
    \right)
    &
    \geq d(n)
\end{align*}
Esempi di algoritmi di questo tipo sono il quicksort randomizzato o gli algoritmi di approssimazione. Quest'ultimi ritornano una soluzione che può non essere ottima, ma è sempre ammissibile.

\subsubsection{Algoritmi Monte Carlo}
% pag 123.3
Gli algoritmi di tipo \emph{Monte Carlo} sono algoritmi randomizzati che \emph{possono} sbagliare.
Dato un algoritmo $
A_{\bpi{}} \left( i \right) \to s
$, si studia la sua probabilità di correttezza $
p_c(n)
$:
\begin{equation*}
    Pr \left( 
        i
        \,
        \bpi{}
        \,
        s
    \right)
    = p_c(n)
\end{equation*}
Anche per algoritmi di questo tipo si analizza il tempo di esecuzione
\begin{align*}
    Pr \left( 
        \bm{T}_A \left( 
            n
        \right)
        \leq c \cdot f(n)
    \right)
    &
    \geq d(n)
\end{align*}
e ci sono casi in cui questo è deterministico, perché il numero di iterazioni non è legato alle scelte casuali effettuate e vale $
d(n) = 1
$.

\subsubsection{Algoritmi Monte Carlo decisionali}
% pag 123.5
Gli algoritmi \emph{Monte Carlo} decisionali possono commettere errori di due tipi:
\begin{itemize}
    \item \emph{one-sided}: possono compiere errori solo su uno dei due \emph{outcome}
    \item \emph{two-sided}: possono compiere errori su entrambi gli \emph{outcome}
\end{itemize}

\subsection{Bound in alta probabilità}
% pag 123.8

\subsubsection{Limite alla complessità in alta probabilità}

\begin{definition}
    [Complessità in alta probabilità]
    \label{def:complessita_alta_prob}
    Dato $
    \bpi{} \subseteq \bi{} \times \bs{}
    $, un algoritmo randomizzato $
    A_{\bpi{}}
    $ ha complessità $
    O \left( f(n) \right)
    $ con alta probabilità se $
    \exists c, d > 0
    $ costanti, $
    \exists n_0
    $ tali che $
    \forall n \geq n_0, 
    \,
    \forall i \in \bi{}_n
    $ vale 
    \begin{equation*}
        Pr \left( 
            T \left( 
                A_{\bpi{}}
                (i)
            \right)
            \geq c f(n)
        \right)
        \leq
        \frac{1}{n^d}
    \end{equation*}
    o alternativamente
    \begin{equation*}
        Pr \left( 
            T \left( 
                A_{\bpi{}}
                (i)
            \right)
            \leq c f(n)
        \right)
        \geq
        1-
        \frac{1}{n^d}
    \end{equation*}
\end{definition}
Note:
\\
$d$ può essere una costante qualsiasi, anche minore di 1, perché è comunque sufficiente a indicare che c'è una sostanziale diminuzione della probabilità all'aumentare della taglia.
\\
Entrambi i metodi per definire l'alta probabilità delimitano la massa della probabilità della coda dell'istanza
\\
L'analisi è al caso peggiore: l'istanza non è scelta casualmente ma è fissata.
La probabilità invece è la probabilità sullo spazio $
\Omega_n
$ delle scelte fatte dall'algoritmo.

\subsubsection{Correttezza in alta probabilità}

\begin{definition}
    [Correttezza in alta probabilità]
    \label{def:correttezza_alta_prob}
    Dato $
    \bpi{} \subseteq \bi{} \times \bs{}
    $, un algoritmo randomizzato $
    A_{\bpi{}}
    $ è corretto in alta probabilità se $
    \exists d > 0
    $ costante, $
    \exists n_0
    $ tali che $
    \forall n \geq n_0, 
    \,
    \forall i \in \bi{}_n
    $ vale 
    \begin{equation*}
        Pr \left( 
            A_{\bpi{}}
            (i)
            \,
            \cancel{
                \bpi{}
            }
            \,
            i
        \right)
        \leq
        \frac{1}{n^d}
    \end{equation*}
\end{definition}
Note:
\\
È un'analisi di probabilità che mostra che l'algoritmo è corretto quasi sempre.
\\
È una caratterizzazione molto più forte della media: la massa di probabilità della coda della distribuzione ha misura che tende a 0.
\\
Il valore di $d$ può spesso essere aumentato molto con facilità: nel caso per esempio di Miller-Rabin, è sufficiente un valore delle iterazioni pari a 
\begin{equation*}
    s = \log_2 n
    \rightarrow
    Pr \left( errore \right)
    =
    \frac{1}{2^s}
    =
    \frac{1}{n}
\end{equation*}
per ottenere la correttezza in alta probabilità.
Aumentando di un fattore costante il numero di iterazioni la probabilità scende però esponenzialmente 
\begin{equation*}
    s = k \log_2 n
    \rightarrow
    Pr \left( errore \right)
    =
    \frac{1}{2^s}
    =
    \frac{1}{n^{k}}
\end{equation*}

\section{Concentration bound per variabili aleatorie}
% pag 124.3

\subsection{Disuguaglianze di Markov}
% pag 124.5
Per ogni variabile aleatoria intera positiva $
\bm{T}
$ vale $
\forall t
$:
\begin{equation*}
    Pr \left( 
        \bm{T} \geq t
    \right)
    \leq
    \frac{
        \E{
            \bm{T}
        }
    }{
        t
    }
\end{equation*}
Se in più $
\bm{T}
$ è limitata superiormente da $b$ con probabilità 1: $
Pr \left( \bm{T} > b \right) = 0
$ vale
\begin{equation*}
    \E{\bm{T}}
    \leq 
    t + 
    \left( 
        b - t
    \right)
    Pr \left( 
        \bm{T} \geq t
    \right)
\end{equation*}

\subsubsection{Confronto tra analisi al caso medio e in alta probabilità}

Se, come è ragionevole supporre in molti casi, l'algoritmo randomizzato ha limite superiore deterministico alla complessità al caso peggiore
\begin{equation*}
    Pr \left( 
        T_A(n) \leq cn^{\alpha}
    \right)
    = 1
\end{equation*}
(per esempio per quicksort randomizzato, anche nel caso peggiore di scelte più sfortunate, la complessità è al massimo quadratica.
Questa analisi lasca mette comunque un limite all'esecuzione, che non può diventare esponenziale).
\\
e si può scegliere $d = d_c$ come funzione crescente di $c$
(si può aumentare il grado del polinomio aumentando la constante moltiplicativa davanti alla complessità),
\begin{equation*}
    Pr \left( 
        T_A(n) \geq cf(n)
    \right)
    \leq
    \frac{1}{n^d}
\end{equation*}
sotto queste condizioni si dimostra che l'analisi al caso medio è meno informativa dell'analisi in alta probabilità.

Usando la seconda disuguaglianza di Markov con $
b = n^{\alpha}
$ e $
t = cf(n)
$, con $
c : d \geq \alpha
$:
\begin{align*}
    \E{
        \bm{T}_A (n)
    }
    &
    \leq
    c f(n)
    +
    \left( 
        n^{\alpha}
        -
        c f(n)
    \right)
    \frac{1}{n^d}
    \\
    &
    \leq
    c f(n)
    +
    \frac{n^{\alpha}}{n^d}
    \\
    &
    \leq
    c f(n)
    +
    1
\end{align*}

\subsection{Bound di Chernoff}
% pag 125.2

Data una generica famiglia di variabili indicatrici
\begin{equation*}
    X_i = 
    \begin{cases}
        1 & \text{con probabilità } p_i
        \\
        0 & \text{con probabilità } 1 - p_i
    \end{cases}
\end{equation*}
Le variabili sono mutualmente indipendenti, e vale $
\E{X_i} = p_i
$.
La variabile di interesse è la somma delle variabili indicatrici
\begin{equation*}
    X = 
    \sum_{i=1}^{n} X_i
\end{equation*}
Per cui vale \begin{equation*}
    \E{X}
    =
    \sum_{i=1}^{n} p_i
    =
    \mu
\end{equation*}

\begin{lemma}
    [Chernoff]
    \label{def:lemma_chernoff}
    Siano $
    X_1, \cdots, X_n
    $ variabili indicatrici con $
    p_i = Pr \left( X_i = 1 \right)
    $. Data $
    X = \sum_{i=1}^{n} X_i
    $ e $
    \mu
    =
    \E{X}
    =
    \sum_{i=1}^{n} p_i
    $, $
    \forall \delta > 0
    $ vale
    \begin{equation*}
        Pr \left( 
            X > \left( 1+ \delta \right) \mu
        \right)
        <
        \left( 
            \frac{
                e^{\delta}
            }{
                \left( 
                    1 + \delta
                \right)^{
                    \left( 
                        1 + \delta
                    \right)
                }
            }
        \right)^{\mu}
    \end{equation*}
\end{lemma}
Note:
\\
Questo lemma fornisce un bound molto più stretto di Markov, infatti 
la frazione è un numero minore di 1 ($
    \left( 
        1 + \delta
    \right)^{
        \left( 
            1 + \delta
        \right)
    }
$ cresce molto più velocemente di $
e^\delta
$) e 
$\mu$ è legato al numero di variabili indicatrici,
quindi c'è un bound esponenziale,
mentre la prima disuguaglianza di Markov risulta
\begin{equation*}
    Pr \left( 
        X > \left( 1+ \delta \right) \mu
    \right)
    < 
    \frac{\mu}{
        \left( 
            1 + \delta
        \right)
        \mu
    }
    =
    \frac{1}{
        1 + \delta
    }
\end{equation*}
che non ha alcun legame con la taglia.
% I bound di \emph{Chernoff} sono più stretti di quelli di Markov, ma valgono solo per variabili aleatorie particolari.

\begin{proof}
    Si considera la famiglia di variabili aleatorie
    \begin{equation*}
        Y_t = e^{tX}
    \end{equation*}
    e ne si studia la media per trovarne una correlazione con la media di $X$.
    Per prima cosa si mostra che
    \begin{align*}
        \E{
            e^{t X}
        }
        &= 
        \E{
            e^{
                t
                \sum_{i=1}^{n} X_i
            }
        }
        \\
        &= 
        \E{
            e^{
                \sum_{i=1}^{n} t X_i
            }
        }
        \\
        &= 
        \E{
            \prod_{i=1}^{n}
            e^{
                t X_i
            }
        }
        \intertext{le variabili $e^{t X_i} $ sono indipendenti, perché le $X_i$ sono indipendenti e la funzione è invertibile, per cui la media del prodotto è il prodotto delle medie}
        &= 
        \prod_{i=1}^{n}
        \E{
            e^{
                t X_i
            }
        }
        \intertext{vale per una singola variabile $
            \E{
                e^{t X_i}
            }
            = 
            e^{t \cdot 0}
            \left( 1 - p_i \right)
            +
            e^{t \cdot 1}
            p_i
            =
            1 + p_i \left( e^t - 1 \right)
            $
        }
        &= 
        \prod_{i=1}^{n}
        \left( 
            1 + p_i \left( e^t - 1 \right)
        \right)
        \intertext{considerando la serie di Taylor troncata vale $
            e^y > 1+y
        $}
        &<
        \prod_{i=1}^{n}
        e^{
            p_i \left( e^t - 1 \right)
        }
        \\
        &= 
        e^{
            \sum_{i=1}^{n}
            p_i \left( e^t - 1 \right)
        }
        \\
        &= 
        e^{
            \left( e^t - 1 \right)
            \sum_{i=1}^{n}
            p_i
        }
        \\
        &= 
        e^{
            \left( e^t - 1 \right)
            \mu
        }
    \end{align*}
    Si sfrutta questo risultato per provare il lemma
    \begin{align*}
        Pr \left( 
            X > \left( 1 + \delta \right) \mu
        \right)
        &= 
        Pr \left( 
            tX > t \left( 1 + \delta \right) \mu
        \right)
        \\
        &= 
        Pr \left( 
            e^{
                tX
            }
            >
            e^{
                t \left( 1 + \delta \right) \mu
            }
        \right)
        \\
        &= 
        Pr \left( 
            Y_t
            >
            e^{
                t \left( 1 + \delta \right) \mu
            }
        \right)
        \intertext{e per la prima disuguaglianza di Markov}
        &< 
        \frac{
            \E{
                Y_t
            }
        }{
            e^{
                t \left( 1 + \delta \right) \mu
            }
        }
        \\
        &= 
        \frac{
            \E{
                e^{tX}
            }
        }{
            e^{
                t \left( 1 + \delta \right) \mu
            }
        }
        \intertext{e per il risultato precedente}
        &<
        \frac{
            e^{
                \left( e^t - 1 \right)
                \mu
            }
        }{
            e^{
                t \left( 1 + \delta \right) \mu
            }
        }
        \intertext{questa è una famiglia di limiti superiori, valida $\forall t > 0$, di cui si seleziona il più stretto che risulta: $
            t_{min} =
            \ln \left( 1 + \delta \right)
            $
        }
        &<
        \frac{
            e^{
                \left(
                    e^{
                        \ln \left( 1 + \delta \right)
                    }
                - 1 \right)
                \mu
            }
        }{
            e^{
                \ln \left( 1 + \delta \right)
                \left( 1 + \delta \right) \mu
            }
        }
        \intertext{chiaramente $
            e^{
                \ln \left( 1 + \delta \right)
            }
            =
            \left( 1 + \delta \right)
            $
        }
        &= 
        \frac{
            e^{
                \left( 
                    1 + \delta - 1
                \right)
                \mu
            }
        }{
            \left( 1 + \delta \right)^{
                \left( 1 + \delta \right) \mu
            }
        }
        \intertext{numeratore e denominatore sono elevati alla $\mu$, e si conclude}
        &= 
        \left( 
            \frac{
                e^{\delta}
            }{
                \left( 
                    1 + \delta
                \right)^{
                    \left( 
                        1 + \delta
                    \right)
                }
            }
        \right)^{\mu}
    \end{align*}
\end{proof}

\begin{corollario}
    Per $
    0 < \delta < 1
    $ valgono i seguenti bound
    \begin{align*}
        Pr \left( 
            X > \left( 1 + \delta \right) \mu
        \right)
        &
        <
        e^{
            -\delta^2 \mu / 3
        }
        \\
        Pr \left( 
            X < \left( 1 - \delta \right) \mu
        \right)
        &
        <
        e^{
            -\delta^2 \mu / 2
        }
    \end{align*}
\end{corollario}
Per cui i valori di $X$ sono estremamente concentrati intorno alla media.
Se si ricava un valore di $d$ per cui $
\delta^2 \mu / 2
> 
d \log n
$ si ottiene l'alta probabilità
\begin{equation*}
    Pr \left( 
        X < \left( 1 - \delta \right) \mu
    \right)
    <
    e^{
        -\delta^2 \mu / 2
    }
    <
    \frac{1}{n^d}
\end{equation*}

% TODO note caso cattivo legato a taglia 8m 4/12



\section{Analisi di Quicksort randomizzato}
pag 128

\subsection{Implementazione}
pag 128

\subsection{Scelta casuale del Pivot}
pag 129.5

\section{Amplificazione della probabilità}
pag 132.6

\section{Algoritmo di Karger}
pag 134.7

\subsection{Multigrafi e contrazioni}
pag 134.7

\section{Pezzi utili di \LaTeX{}}

\subsection{Template vari}

\begin{algorithm}[H]
\caption{Divide and Conquer}\label{alg:dnc}
\begin{algorithmic}[1]
    \Procedure{Divide\&Conquer}{$i$}
        \If{$|i| \leq n_0$}
        \Comment{BASE}
            \State *risolvo direttamente*
        \EndIf
        \State $\langle i_1, i_2, \dots, i_k \rangle \gets A_D(i)$ 
        \Comment{DIVIDE}
        \For{$j \gets 1 $ to $ k $ }
        \Comment{RECURSE}
            \State $s_j \gets $
            \Call{Divide\&Conquer}{$i_j$}
        \EndFor
        \State $s \gets A_C(\langle s_1, s_2, \dots, s_k \rangle)$
        \Comment{CONQUER}
        \State return $s$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
Testo non identato!

\begin{definition}[Algoritmo]\label{def:algex}
    Un algoritmo è una procedura computazionale finita (terminante) e deterministica, specificata come una sequenza di passi elementari (istruzioni) estratte da un insieme standard associato a un modello computazionale (astrazione di un computer) che trasforma in maniera univoca un ingresso in un uscita.
\end{definition}

Guarda che so fare
\begin{equation*}
    \setzo{m}
    \quad
    \setzo{}
    \quad
    \odot
    \quad
    \oplus
    \quad
    \zz{n}
    \quad
    \zs
    \quad
    \zs{\rho(n)}
    \quad
    \equiv_n
\end{equation*}

Un problema
\begin{align*}
    SS: & \\
    \texttt{istanza:} \quad &
    \langle
        S,t
    \rangle
    \\
    \text{dove} \quad &
    S \subseteq \mathbb{N} - \left\{ 0 \right\} \text{ finito}
    \\
    &
    t \subseteq \mathbb{N} - \left\{ 0 \right\} \\
    \texttt{domanda:} \quad &
    \exists \, S' \subseteq S : \sum_{s \in S'}^{} s = t \, ?
\end{align*}

Una lista
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item[--] $L_A = L$ (il linguaggio deciso da $A$ è $L$)
    \item[--] $T_A(|x|) = O(|x|^k)$ per qualche costante $k \geq 0$
\end{itemize}

\subsection{Teoremi e vari ambienti }

\begin{theorem}
    Un teorema senza nome
\end{theorem}

\begin{corollario}
    Un primo corollario
\end{corollario}

\begin{corollario}
    Un secondo corollario
\end{corollario}

\begin{theorem}[Nome del teorema]
    Un teorema con nome
\end{theorem}

\begin{corollario}
    Un primo corollario del teorema con nome
\end{corollario}

\begin{definition}
    Una definizione utile, la seconda di questa sezione
\end{definition}

\begin{proposizione}[Nome della proposizione]
    Una proposizione con nome
\end{proposizione}

\begin{corollario}
    Un primo corollario della proposizione con nome
\end{corollario}

\subsection{Grafi}

Grafi facili da mantenere

% Declare layers (done in main)
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\begin{figure}[h]
    \centering
    \caption{Algoritmo di Prim}
    \label{fig:prim1}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Archi uscenti dal primo nodo}
        \label{fig:au1}
        \begin{tikzpicture} [
                % automagically put labels not on edges
                auto,
                % exchanges the roles of left and right in automatic placement
                % the side the label is put on depends on the order of the nodes in the edge
                swap,
                scale=1.4,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=red!24},
                edge/.style = {draw,thick,-},
                weight/.style = {font=\small},
                selected edge/.style = {draw,line width=5pt,-,red!50},
                outbound edge/.style = {draw,line width=5pt,-,blue!50},
                ignored edge/.style = {draw,line width=5pt,-,black!20}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                {(0,0)/d}, {(0,2)/a}, {(1.5,2)/b}, {(4,2)/c}, {(3,1.2)/e}, {(2,0)/f}, {(4,0)/g}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest /\weight in {b/a/7, c/b/8, d/a/5, d/b/9,
                                                 e/b/7, e/c/5, e/d/15,
                                                 f/d/6, f/e/8, g/e/9, g/f/11}
                \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
            % color a node
            % \path node[selected vertex] at (d) {$d$};
            % prepare it in loop version anyway
            \foreach \vertex in {d}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            % For convenience we use a background layer to highlight edges
            % This way we don't have to worry about the highlighting covering
            % weight labels. 
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {d/a,d/b,d/e,d/f}
                    \path[outbound edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Arco selezionato}
        \label{fig:au2}
        \begin{tikzpicture} [
                auto,
                swap,
                scale=1.4,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=red!24},
                edge/.style = {draw,thick,-},
                weight/.style = {font=\small},
                selected edge/.style = {draw,line width=5pt,-,red!50},
                outbound edge/.style = {draw,line width=5pt,-,blue!50},
                ignored edge/.style = {draw,line width=5pt,-,black!20}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                {(0,0)/d}, {(0,2)/a}, {(1.5,2)/b}, {(4,2)/c}, {(3,1.2)/e}, {(2,0)/f}, {(4,0)/g}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest /\weight in {b/a/7, c/b/8, d/a/5, d/b/9,
                                                 e/b/7, e/c/5, e/d/15,
                                                 f/d/6, f/e/8, g/e/9, g/f/11}
                \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
            % color a node
            \foreach \vertex in {d,a}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {b/a,d/b,d/e,d/f}
                    \path[outbound edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {d/a}
                    \path[selected edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    \\[2pt]
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Secondo arco selezionato}
        \label{fig:au3}
        \begin{tikzpicture} [
                auto,
                swap,
                scale=1.4,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=red!24},
                edge/.style = {draw,thick,-},
                weight/.style = {font=\small},
                selected edge/.style = {draw,line width=5pt,-,red!50},
                outbound edge/.style = {draw,line width=5pt,-,blue!50},
                ignored edge/.style = {draw,line width=5pt,-,black!20}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                {(0,0)/d}, {(0,2)/a}, {(1.5,2)/b}, {(4,2)/c}, {(3,1.2)/e}, {(2,0)/f}, {(4,0)/g}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest /\weight in {b/a/7, c/b/8, d/a/5, d/b/9,
                                                 e/b/7, e/c/5, e/d/15,
                                                 f/d/6, f/e/8, g/e/9, g/f/11}
                \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
            % color a node
            \foreach \vertex in {d,a}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {b/a,d/b,d/e,e/f,f/g}
                    \path[outbound edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {d/a, d/f}
                    \path[selected edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Terzo arco selezionato, notare il primo arco ignorato, interno al $MST$.
            La subcaption può essere lunga assai, e brutte cose non succedono
        }
        \label{fig:au4}
        \begin{tikzpicture} [
                auto,
                swap,
                scale=1.4,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=red!24},
                edge/.style = {draw,thick,-},
                weight/.style = {font=\small},
                selected edge/.style = {draw,line width=5pt,-,red!50},
                outbound edge/.style = {draw,line width=5pt,-,blue!50},
                ignored edge/.style = {draw,line width=5pt,-,black!20}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                {(0,0)/d}, {(0,2)/a}, {(1.5,2)/b}, {(4,2)/c}, {(3,1.2)/e}, {(2,0)/f}, {(4,0)/g}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest /\weight in {b/a/7, c/b/8, d/a/5, d/b/9,
                                                 e/b/7, e/c/5, e/d/15,
                                                 f/d/6, f/e/8, g/e/9, g/f/11}
                \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
            % color a node
            \foreach \vertex in {d,a}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {d/b,d/e,e/f,f/g,b/e,b/c}
                    \path[outbound edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {d/a, d/f, a/b}
                    \path[selected edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {d/b}
                    \path[ignored edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Unweighted graph}
    \label{fig:ug}
    \begin{tikzpicture} [
            auto,
            swap,
            scale=1.4,
            vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
            selected vertex/.style = {vertex, fill=red!24},
            edge/.style = {draw,thick,-},
            weight/.style = {font=\small},
            selected edge/.style = {draw,line width=5pt,-,red!50},
            outbound edge/.style = {draw,line width=5pt,-,blue!50},
            ignored edge/.style = {draw,line width=5pt,-,black!20}
        ]
        % First we draw the vertices
        \foreach \pos/\name in {
                {(0,0)/A},
                {(0,2)/B},
                {(2,2)/C},
                {(4,2)/D},
                {(2,0)/E},
                {(4,0)/F},
                {(6,2)/G}}
            \node[vertex] (\name) at \pos {$\name$};
        % Connect vertices with edges and draw weights
        \foreach \source/ \dest in {
                A/B, B/C, C/E, C/D, E/F, D/E, D/G, F/D}
            \path[edge] (\source) -- (\dest);
        % color a node
        \foreach \vertex in {B, D, E}
            \path node[selected vertex] at (\vertex) {$\vertex$};
        \begin{pgfonlayer}{background}
            \foreach \source / \dest in {B/C}
                \path[outbound edge] (\source.center) -- (\dest.center);
            \foreach \source / \dest in {D/G}
                \path[selected edge] (\source.center) -- (\dest.center);
            \foreach \source / \dest in {F/D}
                \path[ignored edge] (\source.center) -- (\dest.center);
        \end{pgfonlayer}
    \end{tikzpicture}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{$FPW$}
    \label{fig:exbend}
    \begin{tikzpicture} [
            scale=1.4,
            vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
            edge/.style = {draw,thick,-},
            outbound edge/.style = {draw,line width=8pt,-,blue!30},
            selected edge/.style = {draw,line width=8pt,-,red!30},
            arrow edge/.style = {draw,thick,->},
            bent right/.style = {bend right=20},
            bent left/.style = {bend left=20},
        ]
        \foreach \pos/\name in {
        {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
            \node[vertex] (\name) at \pos {$\name$};
        \foreach \source/ \dest in {
                b/c,b/d,a/b,a/e}
            \path[arrow edge] (\source) edge [bent right] (\dest);
        \foreach \source/ \dest in {
                c/b,d/b,b/a,e/a}
            \path[arrow edge] (\source) edge [bent right] (\dest);
        \begin{pgfonlayer}{background}
            \foreach \source / \dest in {a/b}
                \path[outbound edge] (\source) edge [bent right] (\dest);
            \foreach \source / \dest in {b/a}
                \path[selected edge] (\source) edge [bent right] (\dest);
        \end{pgfonlayer}
    \end{tikzpicture}
\end{figure}

% \lipsum{10}

\begin{figure}[!b]
% \begin{figure}[H]
    \centering
    \caption{Algoritmo di Christofides splittato in due pagine}
    \label{fig:mchristofides}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{$T^*$ con i nodi dispari evidenziati}
        \label{fig:mchristmst}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                selected vertex/.style = {vertex, fill=blue!24},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,a/b,a/e}
                \path[edge] (\source) edge (\dest);
            \foreach \vertex in {b,c,d,e}
                \path node[selected vertex] at (\vertex) {$\vertex$};
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Sottografo (completo) indotto dai nodi dispari, e matching perfetto di costo minimo evidenziato}
        \label{fig:mchristmatching}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                selected edge/.style = {draw,line width=5pt,-,blue!24},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,b/e,c/e,c/d,e/d}
                \path[edge] (\source) edge (\dest);
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {b/c,d/e}
                    \path[selected edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
\end{figure}
    % \\[2pt]
% magic to split a figure https://tex.stackexchange.com/a/278748
\begin{figure}[htb]\ContinuedFloat
    \caption{Algoritmo di Christofides splittato in due pagine (cont.)}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Ciclo Euleriano
            $ \langle c,b,d,e,a,b,c \rangle$
        }
        \label{fig:mchristeulertour}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                bent right/.style = {bend right=20},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/d,a/b,a/e,d/e}
                \path[edge] (\source) edge (\dest);
            \foreach \source/ \dest in {
                    c/b,b/c}
                \path[edge] (\source) edge [bent right] (\dest);
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Dopo lo \emph{shortcutting}:
            $ \langle c,b,d,e,a,\cancel{b},c \rangle$
        }
        \label{fig:mchristresult}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                bent right/.style = {bend right=30},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/d,a/e,d/e,b/c}
                \path[edge] (\source) edge (\dest);
            \foreach \source/ \dest in {
                    a/c}
                \path[edge] (\source) edge [bent right] (\dest);
        \end{tikzpicture}
    \end{subfigure}
\end{figure}

\lipsum{10}

\begin{figure}[h]
    \centering
    \caption{$3-CNF-SAT$ to $CLIQUE$}
    \label{fig:3cnfsatex}
    \begin{tikzpicture} [
            scale=1,
            vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
            edge/.style = {draw,thin,-},
            outbound edge/.style = {draw,line width=4pt,-,blue!30},
            selected edge/.style = {draw,line width=4pt,-,red!30},
        ]
        % First we draw the vertices
        \foreach \pos/\name/\label in {
                % top left
                {(-1.5,5)/c/x_3},
                {(-2.5,3.5)/b/x_2},
                {(-3.5,2)/a/\bar{x_1}},
                % top right
                {(1.5,5)/d/x_1},
                {(2.5,3.5)/e/x_2},
                {(3.5,2)/f/\bar{x_3}},
                % bottom row
                {(2,0)/g/\bar{x_3}},
                {(0,0)/h/\bar{x_2}},
                {(-2,0)/i/x_1}}
            \node[vertex] (\name) at \pos {\name $\label$};
        % Connect vertices with edges
        \foreach \source/ \dest in {
                f/g,f/i,
                e/g,e/i,
                d/g,d/h,d/i,
                c/d,c/e,c/h,c/i,
                b/d,b/e,b/f,b/g,b/i,
                a/e,a/f,a/g,a/h}
            \path[edge] (\source) -- (\dest);
        % color some edges on background layer
        \begin{pgfonlayer}{background}
            \foreach \source / \dest in {c/d,d/i,i/c}
                \path[outbound edge] (\source.center) -- (\dest.center);
            \foreach \source / \dest in {a/e,e/g,g/a}
                \path[selected edge] (\source.center) -- (\dest.center);
        \end{pgfonlayer}
    \end{tikzpicture}
\end{figure}

\subsection{Parole in libertà}

Tutte le parole accentate si possono estrarre con

\texttt{grep -rEiohI '[a-z]*(à|è|é|ì|ò|ù)' | sort -u}

\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item \texttt{-r}:
        Read all files under each directory, recursively
    \item \texttt{-E}:
        Interpret PATTERN as an extended regular expression
    \item \texttt{-i}:
        Ignore case distinctions in both the PATTERN and the input files
    \item \texttt{-o}:
        Print only the matched (non-empty) parts of a matching line
    \item \texttt{-h}:
        Suppress the prefixing of file names on output
    \item \texttt{-I}:
        Process a binary file as if it did not contain matching data
\end{itemize}

\texttt{sort -u} ordina l'output ed elimina i duplicati

affidabilità
andrà
associatività
avrà
Bé
biiettività
capacità
cardinalità
cioè
complessità
Complessità
così
difficoltà
Difficoltà
divisibilità
Divisibilità
dovrà
è
È
facilità
farà
finché
generalità
già
identità
Identità
inapprossimabilità
Inapprossimabilità
leggibilità
libertà
massimalità
metà
molteplicità
né
necessità
ottimalità
perché
Perché
però
più
polinomialità
potrà
primalità
probabilità
Probabilità
proprietà
Proprietà
pseudoprimalità
può
Può
qualità
quantità
randomicità
realtà
Riducibilità
sarà
sé
segnerà
Sì
soddisfacibilità
Soddisfacibilità
suriettività
transitività
unicità
Unicità
unità
utilità
validità
velocità
verità
