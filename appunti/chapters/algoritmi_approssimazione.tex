% \section{Algoritmi di approssimazione}

\section{Introduzione}
% Recap problemi di ottimizzazione, pag 42.

\subsection{Problemi di ottimizzazione}
% pag 5

I problemi di ottimizzazione sono definiti su insiemi di istanze e soluzioni $
\bi, \bs
$ per cui esiste una funzione di costo
% In un problema di ottimizzazione, si definisce una funzione di costo associato ad una soluzione:
\begin{equation*}
    c : \bs{} \to \mathbb{R}
\end{equation*}
e, dato l'insieme di soluzioni ammissibili associate ad un'istanza
\begin{equation*}
    \bs{} (i) = \left\{ s \in \bs{} : i \, \bpi{} \, s \right\}
\end{equation*}
si vuole individuare la soluzione di costo massimo (o minimo)
\begin{equation*}
    s_{i}^{*} = \argmax \left\{ c(s) : s \in \bs{} (i) \right\}
\end{equation*}

\subsection{Approcci risolutivi}

\subsubsection{Risoluzioni esaustive}

Si cerca la soluzione esatta, con una ricerca esaustiva efficiente, per esempio con le tecniche \emph{Branch and Bound} o \emph{Branch and Cut}.

\subsubsection{Algoritmi pseudo polinomiali}

Per alcuni problemi, se i dati fossero rappresentati con codifica unaria, l'algoritmo risolutivo sarebbe di complessità polinomiale.

I problemi Strong-NP-Hard restano NP-hard anche sotto encoding unario.

\emph{Subset Sum}, per esempio, si può approcciare come problema di programmazione dinamica (ESERCIZIO).
Si
% definiscono
possono infatti definire 
i sottoproblemi $S_{i, j}$ dove $S_i$ è un prefisso di $S$: $ S_i = \{ s_1, \dots, s_i \} $
e dove $1 \leq j \leq t$.
Questi sottoproblemi sono
% Quanti sono questi sottoproblemi? Ce ne sono
$n \cdot t$, ma $t$ è il numero nell'istanza,
% quanti bit ci sono per rappresentarlo?
che si rappresenta con solamente $\log t$ bit.
Il numero di problemi è quindi esponenziale nella taglia dell'istanza, se $t = 2^{50}$, si genererebbe un numero enorme di problemi, ma il numero viene rappresento con $50$ bit.
La taglia è quindi logaritmica in $t$ ma il numero di problemi è lineare in $t$.
Il numero di problemi sarebbe polinomiale sotto la codifica unaria.
In istanze ragionevoli dove $t$ è piccolo (sul miliardo) si possono risolvere.
\\
L'algoritmo di programmazione dinamica usa una proprietà di sottostruttura che permette di trovare la soluzione $S_{i,j}$ a partire da istanze precedenti $S_{i-1,j}$ con valori di $j$ più piccoli.

Molte istanze piccole di problemi pseudo polinomiali vengono risolte usando la programmazione dinamica.

\subsubsection{Rinunciare all'ottimalità}

Si rinuncia all'ottimalità e si ottiene una soluzione che ha una relazione garantita con la soluzione ottima. Si può \emph{quantificare} di quanto sia peggiore dell'ottimo.

\section{Algoritmi di approssimazione}

\subsection{Definizione}

% Definizione algoritmo di rho-approssimazione, pag 43.
% TODO
\begin{definition}[Algoritmo di approssimazione]
    \label{def:algoapprossimazione}
    Dato $\bpi$ di ottimizzazione,
    $A_{\bpi}$
    è un algoritmo per
    $\bpi$
    che ritorna $
    A_{\bpi} (i) \in \bs (i)
    $
    Si dice che $A_{\bpi}$ è di
    $\rho (n)$-approssimazione
    per $\bpi$ se $\forall i \in \bi, |i|=n$, vale, per $\rho(n) \geq 1$
    \begin{itemize}
        \item problema di \texttt{minimo:} $
            \displaystyle
            \frac{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }{
                c \left( 
                    s^* \left( i \right)
                \right)
            } \leq \rho \left( n \right)
            $
        \item problema di \texttt{massimo:} $
            \displaystyle
            \frac{
                c \left( 
                    s^* \left( i \right)
                \right)
            }{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            } \leq \rho \left( n \right)
            $
    \end{itemize}
    Dove $
        c \left( 
            s^* \left( i \right)
        \right)
    $ è il costo della soluzione ottima.
    Nota: se $A_{\bpi}$ risolve il problema, $\rho(n)=1$.
    Si può riscrivere la maggiorazione in una singola espressione:
    \begin{equation*}
        \max 
        \left\{ 
            \frac{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }{
                c \left( 
                    s^* \left( i \right)
                \right)
            }
            ,
            \frac{
                c \left( 
                    s^* \left( i \right)
                \right)
            }{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }
        \right\}
        \leq \rho \left( n \right)
    \end{equation*}
\end{definition}

\subsubsection{Lower/Upper bound sul costo ottimo}

È interessante notare che un algoritmo di approssimazione fornisca in tempo polinomiale un lower (upper) bound al costo della soluzione ottima, che non è conoscibile in tempo ragionevole.
\begin{itemize}
    \item problema di \texttt{minimo:} $
        \displaystyle
            c \left( 
                s^* \left( i \right)
            \right)
            \geq
            \frac{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }{
                \rho \left( n \right)
            }
        $
    \item problema di \texttt{massimo:} $
        \displaystyle
            c \left( 
                s^* \left( i \right)
            \right)
        \leq
        \rho \left( n \right)
        \cdot
            c \left( 
                A_{\bpi} \left( i \right)
            \right)
        $
\end{itemize}
% TODO commento pag 44.2 su relazione s* s'

\subsubsection{Tipologie di algoritmi}

La 
$\rho (n)$-approssimazione
è un concetto generale, legato alla taglia dell'istanza.

I problemi possono essere approssimati con qualità molto variabile.

Per esempio,
per \emph{Vertex Cover}, si trova un'approssimazione $\rho(n) = 2$ costante.
Per \emph{Set Cover}, la qualità dell'approssimazione è legata alla taglia $\rho(n) = \Theta ( \log n )$.
Per il \emph{Travelling Salesman Problem} in versione generale, e per \emph{Clique}, sotto l'ipotesi $\bp \ne \bnp$, si trova un limite inferiore all'approssimazione $\rho(n) = \Omega ( n^{1-\varepsilon} )$.

\subsection{Schemi di approssimazione}

\begin{definition}[Schema di approssimazione]
    \label{def:schemaapprox}
    L'algoritmo
    $A_{\bpi} (i,\varepsilon)$
    è uno schema di approssimazione
    per $\bpi$ se
    $A_{\bpi} (i,\varepsilon)$
    è di $(1+\varepsilon)$-approssimazione per $\bpi$
\end{definition}

\begin{definition}[PTAS]
    \label{def:ptas}
    L'algoritmo
    $A_{\bpi} (i,\varepsilon)$
    è uno schema di approssimazione polinomiale
    (\emph{polinomial time approximation scheme})
    per $\bpi$ se
    $A_{\bpi} (i,\varepsilon)$
    è di $(1+\varepsilon)$-approssimazione per $\bpi$
    e, fissato $\varepsilon$, ha complessità polinomiale.
\end{definition}

\begin{definition}[FPTAS]
    \label{def:fptas}
    L'algoritmo
    $A_{\bpi} (i,\varepsilon)$
    è uno schema di approssimazione pienamente polinomiale
    (\emph{fully polinomial time approximation scheme})
    per $\bpi$ se
    $A_{\bpi} (i,\varepsilon)$
    è di $(1+\varepsilon)$-approssimazione per $\bpi$
    e, fissato $\varepsilon$, è polinomiale sia in $n$ sia in $1/\varepsilon$.
\end{definition}

La complessità è quindi legata alla taglia e al fattore di approssimazione scelto, e può essere di varie forme, per esempio:
\\
Algoritmi con complessità legata in maniera polinomiale ad $\varepsilon$ e alla taglia $n$ (FPTAS):
% \begin{equation*}
    % T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    % O \left( 
        % \frac{1}{\varepsilon^2} \, n^3
    % \right)
% \end{equation*}
% In questo caso, se si vuole ridurre l'errore relativo di un fattore costante, la complessità cresce di un fattore costante
% \begin{equation*}
    % \frac{1}{\varepsilon^2}
    % \leadsto
    % \frac{1}{\varepsilon^2} \, k^2
% \end{equation*}
% ooooooooooooooooooooooo
\begin{align*}
    T_{A_{\bpi}} \left( n, \varepsilon \right) &= 
    O \left( 
        \frac{1}{\varepsilon^2} \, n^3
    \right)
    \intertext{se si varia l'errore relativo di un fattore $k$ costante, la complessità cresce di un fattore costante}
    \rho = (1+\varepsilon)
    &
    \leadsto
    \rho = \left( 
        1+\frac{\varepsilon}{k}
    \right)
    \\
    T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    O \left( 
        \frac{1}{\varepsilon^2} \, n^3
    \right)
    &
    \leadsto
    O \left( 
        \frac{1}{\varepsilon^2} \, k^2 \, n^3
    \right)
\end{align*}
Altri algoritmi vedono comparire $\varepsilon$ come esponente, in questo caso la complessità \emph{è} polinomiale per $\varepsilon$ fissato, ma cresce rapidamente variando $\varepsilon$ (PTAS):
\begin{align*}
    T_{A_{\bpi}} \left( n, \varepsilon \right) &= 
    O \left( 
        % n^{\frac{1}{\varepsilon}}
        n^{1 / \varepsilon}
    \right)
    \intertext{se si varia l'errore relativo di un fattore $k$}
    \rho = (1+\varepsilon)
    &
    \leadsto
    \rho = \left( 
        1+\frac{\varepsilon}{k}
    \right)
    \\
    T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    O \left( 
        n^{1 / \varepsilon}
    \right)
    &
    \leadsto
    O \left( 
        n^{1 / (\varepsilon/k)}
    \right)
    =
    O \left( 
        \left( n^{1/\varepsilon} \right)^k
    \right)
\end{align*}
Spesso si ottiene una complessità della forma
\begin{equation*}
    T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    O \left( 
        n^h \, 2^{(1/\varepsilon)^k}
    \right)
\end{equation*}
quando si generano in tempo polinomiale delle sottoistanze piccole (di dimensione legata a $\varepsilon$), che poi si risolvono esaustivamente.

\section{Vertex cover}
% pag 45-48

\subsection{Introduzione}

Dato un grafo non orientato, si vuole identificare il \emph{Vertex Cover} di taglia minima.

\subsubsection{Approccio Greedy}

Un primo approccio greedy potrebbe essere quello di scegliere un nodo arbitrario, eliminare gli archi coperti e iterare fino a coprire tutti gli archi.
Per provare che un algoritmo approssima male il problema, è sufficiente esibire un singolo controesempio.
In questo caso se si considera una stella, l'algoritmo potrebbe selezionare tutti gli estremi e non in centro, generando un $VC$ di taglia $n-1$, rispetto alla scelta ottima del singolo nodo centrale.
Il fattore di approssimazione risulta $
\rho (n) = 
(n-1)/1 = n-1
$ che è quasi il peggiore possibile.
Si può pensare di seguire una scelta greedy più astuta, per esempio scegliendo il nodo di grado massimo, ma in questo caso si può provare che risulta un algoritmo con fattore di approssimazione $
\rho (n) = \log n
$.

\subsubsection{Approccio tramite costrutto polinomiale}

Si segue quindi una strada differente: basandosi su qualche costrutto che si riesce a costruire in tempo polinomiale, si ottiene un \emph{Vertex Cover}, e ne si studia la dimensione relativa all'ottimo.

Per un algoritmo di approssimazione vanno studiati
% \begin{itemize}
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item correttezza
    \item complessità
    \item fattore di approssimazione
\end{itemize}

\subsection{Approssimazione tramite \emph{matching} massimale}

Il \emph{matching}, o \emph{independent edge set}, è un insieme di archi senza vertici in comune.
A partire da un \emph{matching} massimale $M$ (ovvero per cui se viene aggiungo un arco, non è più un \emph{matching} valido), si costruisce un \emph{Vertex Cover} che consiste in tutti gli estremi degli archi in $M$.
\begin{algorithm}[H]
\caption{Aprossimatore per Vertex Cover}\label{alg:approxvc}
\begin{algorithmic}[1]
    \Procedure{APPROX\_VC}{$G=(V,E)$}
        \State $V' \gets \emptyset$
        \State $E' \gets E$
        \While{$E' \ne \emptyset$}
            \State * sceglie arco arbitrario $\{ u,v \} \in E$ *
            \State $V' \gets V' \cup \{ u,v \}$
            \label{alg:approxvc:vvuv}
            \State $E' \gets E' - \{ 
                e \in E' : \exists z \in V :
                ( e = \{ u,z \} )
                \vee
                ( e = \{ v,z \} )
            \}
            \label{alg:approxvc:cleanup}
            $
        \EndWhile
        \State return $V'$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
Nota: alla riga \ref{alg:approxvc:vvuv}, $V'$ è un insieme di nodi a cui vengono aggiungi i due nodi nell'arco, che è visto come insieme di due nodi.

\subsubsection{Complessità}

La complessità di questo algoritmo è lineare, $O ( |V| + |E| )$.

\subsubsection{Correttezza}

Per la correttezza, va argomentato che $V' = VC$. Questo è vero, infatti si esce dal ciclo solo quando $E'$ è vuoto, quindi ogni arco $e=\{u,v\}$ viene eliminato, per uno di due motivi:
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item è l'arco scelto arbitrariamente, quindi i suoi nodi sono inseriti in $V'$ (riga  \ref{alg:approxvc:vvuv})
    \item uno dei suoi estremi è parte dell'arco preso in considerazione, ed $e$ viene rimosso nel clean up (riga \ref{alg:approxvc:cleanup})
\end{itemize}
In entrambi i casi, almeno uno degli estremi di ogni arco è in $V'$, che è quindi un \emph{Vertex Cover}.

\subsubsection{Fattore di approssimazione}
% pag 46
Il fattore di approssimazione è dato da
\begin{equation*}
    \rho = \frac{
    |V'|
    }{
    |V^*|
    }
\end{equation*}
di cui si vuole trovare un limite superiore.

Per quanto riguarda $|V'|$:
sia $M \subseteq E$ l'insieme degli archi selezionati da $APPROX\_VC$.
$M$ è un \emph{matching}, per cui nessuna coppia di archi selezionati condivide estremi:
\begin{equation*}
    \forall e_1, e_2 \in M
    \to
    e_1 \cap e_2 = \emptyset
\end{equation*}
La taglia del $VC$ selezionato $V'$ è quindi esattamente il doppio della taglia del \emph{matching}
\begin{equation*}
    |V'| = 2 |M|
\end{equation*}

Per quanto riguarda $|V^*|$:
va trovato un \emph{lower bound} alla taglia del $VC$ ottimo $
|V^*|
$ rispetto a $|M|$.
Gli archi di $M$ sono disgiunti, per cui in ogni $VC$ deve selezionare almeno un nodo per arco:
\begin{equation*}
    |V^*| \geq |A|
\end{equation*}

\subsubsection{Analisi \emph{slack} o \emph{tight}}
%pag 47

\section{Travelling Salesman Problem}

\section{Pezzi utili di \LaTeX{}}
\begin{algorithm}[H]
\caption{Divide and Conquer}\label{alg:dnc}
\begin{algorithmic}[1]
    \Procedure{D\&C}{$i$}
        \If{$|i| \leq n_0$}
        \Comment{BASE}
            \State *risolvo direttamente*
        \EndIf
        \State $\langle i_1, i_2, \dots, i_k \rangle \gets A_D(i)$ 
        \Comment{DIVIDE}
        \For{$j \gets 1 $ to $ k $ }
        \Comment{RECURSE}
            \State $s_j \gets $ \Call{D\&C}{$i_j$}
        \EndFor
        \State $s \gets A_C(\langle s_1, s_2, \dots, s_k \rangle)$
        \Comment{CONQUER}
        \State return $s$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
Testo non identato!

\begin{definition}[Algoritmo]\label{def:algex}
    Un algoritmo è una procedura computazionale finita (terminante) e deterministica, specificata come una sequenza di passi elementari (istruzioni) estratte da un insieme standard associato a un modello computazionale (astrazione di un computer) che trasforma in maniera univoca un ingresso in un uscita.
\end{definition}

Guarda che so fare
\begin{equation*}
    \setzo{m}
    \quad
    \setzo{}
\end{equation*}

Un problema
\begin{align*}
    SS: & \\
    \texttt{istanza:} \quad & \langle S,t \rangle \\
    \text{dove} \quad & S \subseteq \mathbb{N} - \left\{ 0 \right\} \text{ finito} \\
    & t \subseteq \mathbb{N} - \left\{ 0 \right\} \\
    \texttt{domanda:} \quad & \exists \, S' \subseteq S : \sum_{s \in S'}^{} s = t \, ?
\end{align*}

Una lista
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item[--] $L_A = L$ (il linguaggio deciso da $A$ è $L$)
    \item[--] $T_A(|x|) = O(|x|^k)$ per qualche costante $k \geq 0$
\end{itemize}
